{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f378b-b4a2-41d5-b35f-2b97596470a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightkurve as lk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import emcee\n",
    "import corner\n",
    "import speclite as speclite; from speclite import filters\n",
    "from tqdm import tqdm\n",
    "from matplotlib import cm\n",
    "from matplotlib.artist import Artist\n",
    "from chromatic import *\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import curve_fit\n",
    "import sys\n",
    "\n",
    "params = {'legend.fontsize': 'medium',\n",
    "          'figure.figsize': (6, 4),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use('tableau-colorblind10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0338e-4a4a-4796-8e25-a04e78b6b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_photometry(visit='F21',filt_name = 'gp',dt = 0.05*u.day,gap=0.1):\n",
    "   \n",
    "    directory = f'../data/LCO_Photometry/{visit}_phot/'\n",
    "    data = pd.DataFrame(pd.read_pickle(directory+'AU_Mickle.pkl'))\n",
    "\n",
    "    \n",
    "    if visit == 'F21':\n",
    "        if filt_name == 'gp':\n",
    "            filt = data.gp\n",
    "            initial_parameter_guess = [-0.023685, -0.071837, -0.0137700, 0.0113276, 0.0729559, -0.0261908] # these are from an MCMC fit\n",
    "        if filt_name == 'rp':\n",
    "            filt = data.rp \n",
    "            initial_parameter_guess = [-0.02668624, -0.06327924, -0.011149, -0.01762343, 0.0498755, -0.0297356]\n",
    "        if filt_name == 'ip':\n",
    "            filt = data.ip\n",
    "            initial_parameter_guess = [-0.016888352149918486, -0.03699953137505334, 3.9009679346967575e-05, -0.010208728188986133, 0.02783808342310195, -0.02136799403671536]\n",
    "    if visit == 'S22':\n",
    "        if filt_name == 'gp':\n",
    "            filt = data.gp\n",
    "            initial_parameter_guess = [-0.032588410440507753, -0.06761059388999908, 0.011023202255339365, -0.057253442026644986, 0.05715036881485087, 0.008102219788961765, 0.05752551767297014]\n",
    "        if filt_name == 'rp':\n",
    "            filt = data.rp \n",
    "            initial_parameter_guess = [-0.03309734225060938, -0.06690289300817788, 0.011981227215464681, -0.05248070484073764, 0.05568537726412598, 0.007352254284109964, 0.05188364814889117]\n",
    "        if filt_name == 'ip':\n",
    "            filt = None\n",
    "            print('oh no! The S22 visit does not have ip data')\n",
    "\n",
    "    time = np.array(filt.kb24['time [BJD]'])\n",
    "    f = filt.kb24['flux']\n",
    "    flux = np.array(f/np.median(f))\n",
    "    fluxerr = np.array(filt.kb24['flux_err'])\n",
    "    t24,f24,e24 = median_bin(time, flux, fluxerr, gap_definition=gap)\n",
    "\n",
    "    time = np.array(filt.kb26['time [BJD]'])\n",
    "    f = filt.kb26['flux']\n",
    "    flux = np.array(f/np.median(f))\n",
    "    fluxerr = np.array(filt.kb26['flux_err'])\n",
    "    t26,f26,e26 = median_bin(time,flux,fluxerr,gap_definition=gap)\n",
    "\n",
    "    time = np.array(filt.kb29['time [BJD]'])\n",
    "    f = filt.kb29['flux']\n",
    "    flux = np.array(f/np.median(f))\n",
    "    fluxerr = np.array(filt.kb29['flux_err'])\n",
    "    t29,f29,e29 = median_bin(time, flux,fluxerr,gap_definition=gap)\n",
    "\n",
    "    time = np.array(filt.kb88['time [BJD]'])\n",
    "    f = filt.kb88['flux']\n",
    "    flux = np.array(f/np.median(f))\n",
    "    fluxerr = np.array(filt.kb88['flux_err'])\n",
    "    t88,f88,e88 = median_bin(time, flux,fluxerr,gap_definition=gap)\n",
    "    \n",
    "    if visit == 'F21':\n",
    "        T0 = (59455.48 + 2400000.5)\n",
    "        lc24 = lk.LightCurve(time = t24*u.day,flux = f24,\n",
    "                             flux_err = e24)#.fold(period = 4.865*u.day, epoch_time = T0*u.day)\n",
    "        lc26 = lk.LightCurve(time = t26*u.day,flux = f26,\n",
    "                             flux_err = e26)#.fold(period = 4.865*u.day, epoch_time = T0*u.day)\n",
    "        lc29 = lk.LightCurve(time = t29*u.day,flux = f29,\n",
    "                             flux_err = e29)#.fold(period = 4.865*u.day, epoch_time = T0*u.day)\n",
    "        lc88 = lk.LightCurve(time = t88*u.day,flux = f88,\n",
    "                             flux_err = e88)#.fold(period = 4.865*u.day, epoch_time = T0*u.day)\n",
    "        lcs = [lc24, lc26, lc29, lc88]\n",
    "        labels = ['LCO Siding Spring (kb24)', 'LCO CTIO (kb26)', 'LCO Teide (kb29)', 'LCO Siding Spring (kb88)']\n",
    "        cam_names = ['kb24', 'kb26', 'kb29', 'kb88']\n",
    "        colors = ['#fac205','#0165fc','#8c000f','#ed0dd9']\n",
    "    if visit == 'S22':\n",
    "        T0 = (59455.48 + 2400000.5) + (27.*8.4631427)\n",
    "        time = np.array(filt.kb87['time [BJD]'])\n",
    "        f = filt.kb87['flux']\n",
    "        flux = np.array(f/np.median(f))\n",
    "        fluxerr = np.array(filt.kb87['flux_err'])\n",
    "        t87,f87,e87 = median_bin(time, flux,fluxerr,gap_definition=gap)\n",
    "        lc24 = lk.LightCurve(time = t24*u.day,flux = f24,\n",
    "                             flux_err = e24)#.fold(period = 4.865*u.day, epoch_time = T0*u.day)\n",
    "        lc26 = lk.LightCurve(time = t26*u.day,flux = f26,\n",
    "                             flux_err = e26)#.fold(period = 4.865*u.day, epoch_time = T0*u.day)\n",
    "        lc29 = lk.LightCurve(time = t29*u.day,flux = f29,\n",
    "                             flux_err = e29)#.fold(period = 4.865*u.day, epoch_time = T0*u.day)\n",
    "        lc87 = lk.LightCurve(time = t87*u.day,flux = f87,\n",
    "                             flux_err = e87)#.fold(period = 4.865*u.day, epoch_time = T0*u.day)\n",
    "        lc88 = lk.LightCurve(time = t88*u.day,flux = f88,\n",
    "                             flux_err = e88)#.fold(period = 4.865*u.day, epoch_time = T0*u.day)\n",
    "        lcs = [lc24, lc26, lc29, lc87, lc88]\n",
    "        labels = ['LCO Siding Spring kb24', 'LCO CTIO kb26', 'LCO Teide kb29', 'LCO Sutherland kb87', 'LCO Siding Spring kb88']\n",
    "        cam_names = ['kb24', 'kb26', 'kb29', 'kb87', 'kb88']\n",
    "        colors = ['#fac205','#0165fc','#8c000f','#10a674','#ed0dd9']\n",
    "\n",
    "    return lcs, labels, T0, colors, initial_parameter_guess, cam_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fdbc41-50a1-4174-a131-372ac4b3518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_clip(lightcurve, parameters, sigma = 10, k=0, errval = 0.5, **kwargs):\n",
    "    \n",
    "    lc = lightcurve\n",
    "    z_ = lc.flux\n",
    "    e_ = lc.flux_err\n",
    "    t_ = lc.time.value\n",
    "    \n",
    "    errmask = np.abs(e_) < errval\n",
    "    z = z_[errmask]\n",
    "    e = e_[errmask]\n",
    "    t = t_[errmask]\n",
    "    \n",
    "    model = sinusoid_model(t, parameters)+parameters[k+2]\n",
    "    sigmamask = np.abs(z - model) < sigma*e\n",
    "    \n",
    "    clippedtime = t[sigmamask]\n",
    "    clippedflux = z[sigmamask]\n",
    "    clippederr = e[sigmamask]\n",
    "    \n",
    "    newlc = lk.LightCurve(time = clippedtime*u.day, flux = clippedflux, flux_err = clippederr)\n",
    "\n",
    "    return newlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900f58a-a3a0-46ba-87bd-93c2b7b9fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoid_model(time, parameters,Period = 4.863):\n",
    "        \n",
    "    A = parameters[0]\n",
    "    B = parameters[1]\n",
    "    \n",
    "    argument = (2.0 * np.pi * time)/Period\n",
    "    \n",
    "    model = A*np.sin(argument) + B*np.cos(argument) + 1\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31026438-2d7d-482e-bb33-4c7d534b53f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoid_model_curvefit(time, A, B,Period = 4.863):\n",
    "            \n",
    "    argument = (2*np.pi*time)/Period\n",
    "    model = A*np.sin(argument) + B*np.cos(argument) + 1\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce46082-e2bd-48e9-a861-79360f52a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_chisq(lightcurves,parameters):\n",
    "    \n",
    "    A = parameters[0]\n",
    "    B = parameters[1]\n",
    "    offsets = np.array(parameters[2:])\n",
    "    \n",
    "    chisq = 0\n",
    "    ln_like = 0\n",
    "\n",
    "    for i in range(len(lightcurves)):\n",
    "        err_weight = np.sum(1/np.sqrt(2*np.pi*(lightcurves[i].flux_err)))\n",
    "        _model = sinusoid_model(lightcurves[i].time.value, parameters) + offsets[i]\n",
    "        _chisq = np.sum((lightcurves[i].flux - _model)**2/(lightcurves[i].flux_err)**2)\n",
    "        chisq+=_chisq\n",
    "        ln_like += (err_weight - 0.5*_chisq)\n",
    "\n",
    "    return chisq, ln_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d55f5e-68e5-40c3-abc4-f5e6cc3e20a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob(parameters,lightcurves, **kwargs): #this will return a total ln_like for all 4 cameras\n",
    "    \n",
    "    ln_like = combined_chisq(lightcurves,parameters)[1]\n",
    "    \n",
    "    return ln_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff8c76-0188-4de7-9e6c-cfcbfd0d7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob_formcmc(parameters, **kwargs): #this will return a total ln_like for all 4 cameras\n",
    "    \n",
    "    ln_like = 0\n",
    "    \n",
    "    lightcurves = clipped_lcs\n",
    "\n",
    "    A = parameters[0]\n",
    "    B = parameters[1]\n",
    "    offsets = np.array(parameters[2:])\n",
    "    \n",
    "    for i in range(0,ncams):\n",
    "        model = sinusoid_model(lightcurves[i].time.value,parameters) + offsets[i]\n",
    "        err_weight = np.sum(1/np.sqrt(2*np.pi*(lightcurves[i].flux_err)))\n",
    "        chisq = np.sum((lightcurves[i].flux - model)**2/(lightcurves[i].flux_err)**2)\n",
    "        ln_like += (err_weight - 0.5*chisq)\n",
    "    \n",
    "    return ln_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4e4d4-9e63-472f-a82e-3a221b01ecdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def median_bin(time, flux, uncertainty, gap_definition=0.5, visualize=False):\n",
    "    '''\n",
    "    Group data into chunks, and median-bin them within those chunks.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    time : array \n",
    "        The original times, in days. \n",
    "    flux : array \n",
    "        The original fluxes.   \n",
    "    uncertainty : array \n",
    "        The original uncertainties. \n",
    "    gap_definition : float \n",
    "        If times are separated by more than this many days,\n",
    "        consider them separate clumps. A default of 0.5 days \n",
    "        would naturally.\n",
    "    visualize : bool \n",
    "        Should we make some visualizations showing details of \n",
    "        the binning process or not? \n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    binned_time : array \n",
    "        The binned times.\n",
    "    binned_flux : array \n",
    "        The binned fluxes. \n",
    "    binned_uncertainty : array \n",
    "        The binned_fluxes.\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    - Sort the data in time order. '''\n",
    "    indices = time.argsort(axis=None)\n",
    "    _f = flux[indices]\n",
    "    _e = uncertainty[indices]\n",
    "    _t = time[indices] # _t is now a sorted array of times\n",
    "    \n",
    "    '''\n",
    "    - Identify gap_start as the array indices where np.diff(time) > gap_definition.'''\n",
    "    gap_start = np.diff(_t) >= gap_definition # this is a list of T/F values\n",
    "\n",
    "    '''\n",
    "    - Create an array of clump start times and clump end times. '''\n",
    "    edges = _t[1:]*gap_start # This replaces all but the 'border' values with 0\n",
    "    condition = np.where(edges == 0.) # Array of indices\n",
    "    compressed = np.delete(edges,condition) - 0.5*gap_definition # Removing the values which are not bin edges (which were 0) and offsetting the edge from the points\n",
    "    _edges = np.insert(compressed,0,(_t[0] - 0.5*gap_definition)) # create a first edge\n",
    "    bin_edges = np.append(_edges,np.max(_t) + 0.5*gap_definition) # create a last edge\n",
    "    if visualize:\n",
    "        plt.figure(figsize=(5,4))\n",
    "        plt.title('median binning results')\n",
    "        # plt.xlim(2459455.65,2459455.7)\n",
    "        plt.errorbar(_t, _f, _e, label='sorted & unbinned', \n",
    "                 marker='.', linewidth=0, elinewidth=1, color='gray')\n",
    "        for i in range(len(bin_edges)):\n",
    "            plt.axvline(bin_edges[i],color='k')\n",
    "            if i == 0:\n",
    "                plt.axvline(bin_edges[i],color='k',label='bin edges')\n",
    "    \n",
    "    '''    \n",
    "    - Create empty arrays (binned_time, binned_flux, binned_uncertainty) with one element per clump.'''\n",
    "    binned_time = np.array([None]*(len(bin_edges)-1)) # the number of binned times is 1 more than the number of bin_edges\n",
    "    binned_flux = np.array([None]*(len(bin_edges)-1)) # unless bin_edges includes upper and lower bounds\n",
    "    binned_uncertainty = np.array([None]*(len(bin_edges)-1))\n",
    "\n",
    "    # Loop through clumps, select the data points in each clump.\n",
    "    for i in range(len(bin_edges)-1): \n",
    "\n",
    "        start_time = bin_edges[i]\n",
    "        end_time = bin_edges[i+1]\n",
    "        def_low = np.array(_t > start_time)\n",
    "        def_high = np.array(_t < end_time)\n",
    "        trange = (def_low * def_high)\n",
    "        \n",
    "        these_times = _t*trange\n",
    "        condition = (these_times == 0)\n",
    "        this_clumps_times = np.delete(these_times, condition)\n",
    "        these_fluxes = _f*trange\n",
    "        condition = (these_fluxes == 0)\n",
    "        this_clumps_fluxes = np.delete(these_fluxes, condition)\n",
    "        these_errs = _e*trange\n",
    "        condition = (these_errs == 0)\n",
    "        this_clumps_uncertainties = np.delete(these_errs, condition)\n",
    "        \n",
    "        binned_time[i] = np.median(this_clumps_times) # Set the binned_time for this clump to be the median of the times in the clump.\n",
    "        binned_flux[i] = np.median(this_clumps_fluxes) # Set the binned_flux for this clump to be median of the fluxes in the clump.\n",
    "        binned_uncertainty[i] = astropy.stats.mad_std(this_clumps_fluxes)/np.sqrt(len(this_clumps_times)) #  `astropy.stats.mad_std` to be less sensitive than `np.std` to outliers.\n",
    "        #  # To avoid problems with small numbers of points in a bin, don't let uncertainty be smaller than...\n",
    "        if binned_uncertainty[i] <= np.median(this_clumps_uncertainties)/np.sqrt(len(this_clumps_times)) :\n",
    "            binned_uncertainty[i] = np.median(this_clumps_uncertainties)/np.sqrt(len(this_clumps_times))\n",
    "        if binned_uncertainty[i] <= 0.01:\n",
    "            binned_uncertainty[i] = 0.01\n",
    "    \n",
    "    if visualize:\n",
    "        plt.errorbar(binned_time, binned_flux, binned_uncertainty,\n",
    "                     label='binned', linewidth=0,elinewidth=1, color='red',zorder=20)\n",
    "        plt.legend()\n",
    "    \n",
    "    return binned_time, binned_flux, binned_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e72775-d4e1-401b-beba-3582f502d8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def median_bin(time, flux, uncertainty, gap_definition=0.5, visualize=False):\n",
    "    '''\n",
    "    Group data into chunks, and median-bin them within those chunks.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    time : array \n",
    "        The original times, in days. \n",
    "    flux : array \n",
    "        The original fluxes.   \n",
    "    uncertainty : array \n",
    "        The original uncertainties. \n",
    "    gap_definition : float \n",
    "        If times are separated by more than this many days,\n",
    "        consider them separate clumps. A default of 0.5 days \n",
    "        would naturally.\n",
    "    visualize : bool \n",
    "        Should we make some visualizations showing details of \n",
    "        the binning process or not? \n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    binned_time : array \n",
    "        The binned times.\n",
    "    binned_flux : array \n",
    "        The binned fluxes. \n",
    "    binned_uncertainty : array \n",
    "        The binned_fluxes.\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    - Sort the data in time order. '''\n",
    "    indices = time.argsort(axis=None)\n",
    "    _f = flux[indices]\n",
    "    _e = uncertainty[indices]\n",
    "    _t = time[indices] # _t is now a sorted array of times\n",
    "    \n",
    "    '''\n",
    "    - Identify gap_start as the array indices where np.diff(time) > gap_definition.'''\n",
    "    gap_start = np.diff(_t) >= gap_definition # this is a list of T/F values\n",
    "\n",
    "    '''\n",
    "    - Create an array of clump start times and clump end times. '''\n",
    "    edges = _t[1:]*gap_start # This replaces all but the 'border' values with 0\n",
    "    condition = np.where(edges == 0.) # Array of indices\n",
    "    compressed = np.delete(edges,condition) - 0.5*gap_definition # Removing the values which are not bin edges (which were 0) and offsetting the edge from the points\n",
    "    _edges = np.insert(compressed,0,(_t[0] - 0.5*gap_definition)) # create a first edge\n",
    "    bin_edges = np.append(_edges,np.max(_t) + 0.5*gap_definition) # create a last edge\n",
    "    if visualize:\n",
    "        plt.figure(figsize=(5,4))\n",
    "        plt.title('median binning results')\n",
    "        # plt.xlim(2459455.65,2459455.7)\n",
    "        plt.errorbar(_t, _f, _e, label='sorted & unbinned', \n",
    "                 marker='.', linewidth=0, elinewidth=1, color='gray')\n",
    "        for i in range(len(bin_edges)):\n",
    "            plt.axvline(bin_edges[i],color='k')\n",
    "            if i == 0:\n",
    "                plt.axvline(bin_edges[i],color='k',label='bin edges')\n",
    "    \n",
    "    '''    \n",
    "    - Create empty arrays (binned_time, binned_flux, binned_uncertainty) with one element per clump.'''\n",
    "    binned_time = np.array([None]*(len(bin_edges)-1)) # the number of binned times is 1 more than the number of bin_edges\n",
    "    binned_flux = np.array([None]*(len(bin_edges)-1)) # unless bin_edges includes upper and lower bounds\n",
    "    binned_uncertainty = np.array([None]*(len(bin_edges)-1))\n",
    "\n",
    "    # Loop through clumps, select the data points in each clump.\n",
    "    for i in range(len(bin_edges)-1): \n",
    "        start_time = bin_edges[i]\n",
    "        end_time = bin_edges[i+1]\n",
    "        def_low = np.array(_t > start_time)\n",
    "        def_high = np.array(_t < end_time)\n",
    "        trange = (def_low * def_high)\n",
    "        \n",
    "        these_times = _t*trange\n",
    "        condition = (these_times == 0)\n",
    "        this_clumps_times = np.delete(these_times, condition)\n",
    "        these_fluxes = _f*trange\n",
    "        condition = (these_fluxes == 0)\n",
    "        this_clumps_fluxes = np.delete(these_fluxes, condition)\n",
    "        these_errs = _e*trange\n",
    "        condition = (these_errs == 0)\n",
    "        this_clumps_uncertainties = np.delete(these_errs, condition)\n",
    "        \n",
    "        binned_time[i] = np.median(this_clumps_times) # Set the binned_time for this clump to be the median of the times in the clump.\n",
    "        binned_flux[i] = np.median(this_clumps_fluxes) # Set the binned_flux for this clump to be median of the fluxes in the clump.\n",
    "        binned_uncertainty[i] = astropy.stats.mad_std(this_clumps_fluxes)/np.sqrt(len(this_clumps_times)) #  `astropy.stats.mad_std` to be less sensitive than `np.std` to outliers.\n",
    "        #  # To avoid problems with small numbers of points in a bin, don't let uncertainty be smaller than...\n",
    "        if binned_uncertainty[i] <= np.median(this_clumps_uncertainties)/np.sqrt(len(this_clumps_times)) :\n",
    "            binned_uncertainty[i] = np.median(this_clumps_uncertainties)/np.sqrt(len(this_clumps_times))\n",
    "        if binned_uncertainty[i] <= 0.01:\n",
    "            binned_uncertainty[i] = 0.01\n",
    "    \n",
    "    if visualize:\n",
    "        plt.errorbar(binned_time, binned_flux, binned_uncertainty,\n",
    "                     label='binned', linewidth=0,elinewidth=1, color='red',zorder=20)\n",
    "        plt.legend()\n",
    "    \n",
    "    return binned_time, binned_flux, binned_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dee664-7ddf-4f8e-b823-b8f22bc8b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = ['F21','S22']\n",
    "sigmas = [10,5]\n",
    "\n",
    "for visit in tqdm(visits):\n",
    "    \n",
    "    if visit == 'F21':\n",
    "        filt_names=['gp','rp','ip']\n",
    "    \n",
    "    if visit == 'S22':\n",
    "        filt_names=['gp','rp']\n",
    "        \n",
    "    for this_filter in tqdm(filt_names):\n",
    "\n",
    "        # Set initial parameters for optimization and binning\n",
    "        lcs, labels, T0, colors, initial_parameter_guess, cam_names = initialize_photometry(visit=visit,filt_name=this_filter,gap=0.01)\n",
    "        ndim = len(initial_parameter_guess)  \n",
    "        ncams = ndim - 2\n",
    "        clipped_lcs = [None]*ncams\n",
    "        hires_model_times = np.linspace(T0-20,T0+80,10000)\n",
    "\n",
    "        '''\n",
    "        PLOT 1 - UNCLIPPED DATA, INITIAL MODEL\n",
    "        '''\n",
    "        i = 0\n",
    "        for lc in lcs:\n",
    "            # lc.time = lc.time + (T0*u.day) # Re-orienting the time axis from being a phase\n",
    "            this_model = sinusoid_model(lc.time.value,initial_parameter_guess)+initial_parameter_guess[i+2]\n",
    "            plt.figure()\n",
    "            plt.title(f'{visit} {this_filter} {cam_names[i]} binned photometry (PLOT 1)')\n",
    "            plt.errorbar(lc.time.value,lc.flux,yerr=lc.flux_err,color='red',fmt='o', label = cam_names[i],zorder=-10)\n",
    "            plt.plot(lc.time.value,this_model,label='initial model',color='blue',zorder=-100) #plot the initial models\n",
    "            plt.xlim(lc.time.value[0]-5,lc.time.value[-1]+5)\n",
    "            plt.axvline(T0,label='T0')\n",
    "            plt.ylim(np.min(lc.flux)-0.05,np.max(lc.flux)+0.05)\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.show()\n",
    "            i += 1  \n",
    "            \n",
    "        confirm = input(f\"proceed to clip {sigmas[0]}-sigma outliers? y/n\")\n",
    "        if confirm == 'y':\n",
    "            print(\"Proceeding\")\n",
    "        if confirm == 'n':\n",
    "            print(\"Exiting\")\n",
    "            sys.exit()\n",
    "        '''\n",
    "        FIRST ROUND SIGMA CLIP based on the initial fit\n",
    "        PLOT 2 - UNCLIPPED DATA, ONCE-CLIPPED DATA, INITIAL MODEL\n",
    "        '''\n",
    "        i = 0\n",
    "        for lc in lcs:\n",
    "            clipped = sigma_clip(lc, initial_parameter_guess,sigma=sigmas[0],k=i)\n",
    "            hires_model = sinusoid_model(hires_model_times, initial_parameter_guess) + initial_parameter_guess[i+2]\n",
    "            clipped_lcs[i] = clipped\n",
    "\n",
    "            plt.figure()\n",
    "            plt.title(f'{visit} {this_filter} {cam_names[i]} with {sigmas[0]}-sigma outliers clipped (PLOT 2)')\n",
    "            plt.errorbar(lc.time.value,lc.flux,yerr=lc.flux_err,color='red',fmt='o',label = f'{cam_names[i]} (pre-clip)',zorder=-10)\n",
    "            plt.errorbar(clipped.time.value,clipped.flux,yerr=clipped.flux_err,color='k',fmt='o',label=f'{cam_names[i]} (post-clip)',zorder=10) #plot the initial models\n",
    "            plt.plot(hires_model_times,hires_model,label='hires init model',zorder=-15,alpha=1,color='purple') #plot the initial models\n",
    "            plt.xlim(lc.time.value[0]-5,lc.time.value[-1]+5)\n",
    "            plt.axvline(T0,label='T0')\n",
    "            plt.ylim(np.min(lc.flux)-0.05,np.max(lc.flux)+0.05)\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.show()\n",
    "            i += 1\n",
    "            \n",
    "        confirm = input(f\"Optimize the fit and clip {sigmas[1]}-sigma outliers? y/n\")\n",
    "        if confirm == 'y':\n",
    "            print(\"Proceeding\")\n",
    "        if confirm == 'n':\n",
    "            print(\"Exiting\")\n",
    "            sys.exit()\n",
    "        '''\n",
    "        RUN AN OPTIMIZATION to improve fit parameters\n",
    "        '''\n",
    "        def negative_lnprob(parameters, **kwargs):\n",
    "                return -lnprob(parameters,clipped_lcs)\n",
    "        function_to_minimize = negative_lnprob\n",
    "        results = minimize(function_to_minimize, initial_parameter_guess)\n",
    "        fitted_parameters = results['x']\n",
    "        offsets = fitted_parameters[2:]\n",
    "        p_naught = [fitted_parameters[0],fitted_parameters[1]]\n",
    "        \n",
    "        '''\n",
    "        SECOND ROUND SIGMA CLIP based on the optimized fit\n",
    "        PLOT 3 - UNCLIPPED, TWICE-CLIPPED, and OPTIMIZED MODEL\n",
    "        '''\n",
    "        i = 0\n",
    "        n_obs=0 # this is for the error adjustment \n",
    "        for lc in clipped_lcs:\n",
    "            # optmodel = sinusoid_model(lc.time.value,fitted_parameters) + offsets[i]\n",
    "            clipped = sigma_clip(lc, fitted_parameters,sigma=sigmas[1],k=i)\n",
    "            hires_optimized_model = sinusoid_model(hires_model_times, fitted_parameters) + offsets[i]\n",
    "            clipped_lcs[i] = clipped\n",
    "\n",
    "            plt.figure()\n",
    "            plt.title(f'{visit} {this_filter} {cam_names[i]} with {sigmas[1]}-sigma outliers clipped (PLOT 3)')\n",
    "            plt.errorbar(lcs[i].time.value,lcs[i].flux,yerr=lcs[i].flux_err,color='red',fmt='o',label = f'{cam_names[i]} (pre-clip)',zorder=-10)\n",
    "            plt.errorbar(clipped.time.value,clipped.flux,yerr=clipped.flux_err,color='k',fmt='o',label=f'{cam_names[i]} (post-clip)',zorder=10) \n",
    "            plt.plot(hires_model_times,hires_optimized_model,label='hires optimized model',zorder=-15,alpha=1,color='purple') #plot the initial models\n",
    "            plt.xlim(lc.time.value[0]-5,lc.time.value[-1]+5)\n",
    "            plt.axvline(T0,label='T0')\n",
    "            plt.ylim(np.min(lc.flux)-0.05,np.max(lc.flux)+0.05)\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.show()\n",
    "            n_obs+=len(lc.time)\n",
    "            i += 1\n",
    "            \n",
    "        confirm = input(f\"proceed to re-optimize the model and adjust uncertainties? y/n\")\n",
    "        if confirm == 'y':\n",
    "            print(\"Proceeding\")\n",
    "        if confirm == 'n':\n",
    "            print(\"Exiting\")\n",
    "            sys.exit()\n",
    "        '''\n",
    "        RE-OPTIMIZE THE TWICE-CLIPPED DATA\n",
    "        '''\n",
    "        newresults = minimize(function_to_minimize, fitted_parameters)\n",
    "        newfitted_parameters = results['x']\n",
    "        np.save(f'../data/LCO_Photometry/{visit}_{this_filter}_fittedparameters.npy',newfitted_parameters)\n",
    "        offsets = newfitted_parameters[2:]\n",
    "        p_naught = [newfitted_parameters[0],newfitted_parameters[1]]\n",
    "        '''\n",
    "        ADJUST UNCERTAINTIES on CLIPPED DATA based on NEW OPTIMIZATION\n",
    "        PLOT 4 - UNCLIPPED, TWICE-CLIPPED with NEW UNCERTAINTIES, and NEW MODEL\n",
    "        '''\n",
    "        i = 0\n",
    "        for lc in clipped_lcs:\n",
    "            optimized_model = sinusoid_model(lc.time.value,newfitted_parameters) + offsets[i]\n",
    "            hires_optimized_model = sinusoid_model(hires_model_times, newfitted_parameters) + offsets[i]\n",
    "            rchi2 = np.sum(((lc.flux-optimized_model)**2/lc.flux_err**2))/(n_obs-ndim)\n",
    "            if rchi2 >= 1:\n",
    "                lc.flux_err = lc.flux_err * np.sqrt(rchi2)            \n",
    "\n",
    "            plt.figure()\n",
    "            plt.title(f'{visit} {this_filter} {cam_names[i]} FINAL OPTIMIZED PARAMS (PLOT 4)')\n",
    "            plt.errorbar(lcs[i].time.value,lcs[i].flux,yerr=lcs[i].flux_err,color='r',fmt='o',label = f'{cam_names[i]} (pre-clip)',zorder=-10)\n",
    "            plt.errorbar(lc.time.value,lc.flux,yerr=lc.flux_err,color='k',fmt='o',label = f'{cam_names[i]} (post-clip)',zorder=10) \n",
    "            plt.plot(hires_model_times,hires_optimized_model,label='hires optimized model',zorder=-15,alpha=1,color='purple') #plot the initial models\n",
    "            plt.xlim(lc.time.value[0]-5,lc.time.value[-1]+5)\n",
    "            plt.axvline(T0,label='T0')\n",
    "            plt.ylim(np.min(lc.flux)-0.05,np.max(lc.flux)+0.05)\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.show()\n",
    "            \n",
    "            np.save(f'../data/LCO_Photometry/{visit}_{this_filter}_{cam_names[i]}_times.npy',lc.time.value)\n",
    "            np.save(f'../data/LCO_Photometry/{visit}_{this_filter}_{cam_names[i]}_fluxes.npy',lc.flux.value)\n",
    "            np.save(f'../data/LCO_Photometry/{visit}_{this_filter}_{cam_names[i]}_uncertainties.npy',lc.flux_err.value)\n",
    "    \n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a2407-693c-4f0b-bcb4-99dcc76ffdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cfdcc8-077e-4e36-bcde-2e2b66b7f5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
